{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning on SQuAD evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import collections\n",
    "from io import open\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                          BertForQuestionAnswering, BertTokenizer)\n",
    "# from transformers import AdamW, WarmupLinearSchedule\n",
    "from transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The follwing import is the official SQuAD evaluation script (2.0).\n",
    "# You can remove it from the dependencies if you are using this script outside of the library\n",
    "# We've added it here for automated tests (see examples/test_examples.py file)\n",
    "from utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments(object):\n",
    "    do_eval = True  # 测试模型\n",
    "    # 模型参数\n",
    "    model_type = 'bert'\n",
    "    config_name = ''  # Pretrained config name or path if not the same as model_name\n",
    "    model_name_or_path = 'bert-base-cased'  # Path to pre-trained model or shortcut name\n",
    "    tokenizer_name = ''  # Pretrained tokenizer name or path if not the same as model_name\n",
    "    do_lower_case = True  # Set this flag if you are using an uncased model.\n",
    "    \n",
    "    # 数据参数\n",
    "    train_file = './data/SQuAD/train-v1.1.json'  # 训练集文件路径\n",
    "    predict_file = './data/SQuAD/dev-v1.1.json'  # 测试集文件路径\n",
    "    version_2_with_negative = False  # If true, the SQuAD examples contain some that do not have an answer.\n",
    "    # The maximum total input sequence length after WordPiece tokenization. Sequences. \n",
    "    # longer than this will be truncated, and sequences shorter than this will be padded.\"\n",
    "    max_seq_length = 384\n",
    "    doc_stride = 128  # When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "    max_query_length = 64  # The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\n",
    "    \n",
    "    overwrite_cache = True  # Overwrite the cached training and evaluation sets\n",
    "    \n",
    "    # 训练参数\n",
    "    learning_rate = 5e-5  # The initial learning rate for Adam.\n",
    "    train_batch_size = 2  # batch size train\n",
    "    eval_batch_size = 2  # batch size eval\n",
    "    gradient_accumulation_steps = 1  # Number of updates steps to accumulate before performing a backward/update pass.\n",
    "    max_steps = -1  # If > 0: set total number of training steps to perform. Override num_train_epochs.\n",
    "    num_train_epochs = 3  # Total number of training epochs to perform.\n",
    "    weight_decay = 0.0  # Weight deay if we apply some.\n",
    "    warmup_steps = 0.0  # Linear warmup over warmup_steps.\n",
    "    logging_steps = 50  # Log every X updates steps.\n",
    "    evaluate_during_training = False  # Rul evaluation during training at each logging step.\n",
    "    save_steps = 50  # Save checkpoint every X updates steps.\n",
    "    \n",
    "    output_dir = '/tmp/mrpc_output/'  # The output directory where the model checkpoints and predictions will be written.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 测试使用\n",
    "    n_best_size = 20  # The total number of n-best predictions to generate in the nbest_predictions.json output file.\n",
    "    max_answer_length = 30  # The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.\n",
    "    verbose_logging = True  # If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.\n",
    "    null_score_diff_threshold = 0.0  # If null_score - best_non_null is greater than the threshold predict null.\n",
    "    \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建 SQuAD 样本类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \"\"\"SQuAD样本\n",
    "    A single training/test example for the Squad dataset.\n",
    "    For examples without an answer, the start and end position are -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.qas_id = qas_id  # id\n",
    "        self.question_text = question_text  # 问题\n",
    "        self.doc_tokens = doc_tokens  # 段落\n",
    "        self.orig_answer_text = orig_answer_text  # 答案\n",
    "        self.start_position = start_position  # 答案起始位置\n",
    "        self.end_position = end_position  # 答案终止位置\n",
    "        self.is_impossible = is_impossible  # 是否一定有答案\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \", question_text: %s\" % (\n",
    "            self.question_text)\n",
    "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.start_position:\n",
    "            s += \", start_position: %d\" % (self.start_position)\n",
    "        if self.end_position:\n",
    "            s += \", end_position: %d\" % (self.end_position)\n",
    "        if self.is_impossible:\n",
    "            s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取文件，构建样本列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad_examples(input_file, is_training, version_2_with_negative):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample. 读取 SQuAD json文件，返回一个 SquadExample 列表 list\n",
    "    input_file：json 文件路径\n",
    "    is_training：是否用于训练\n",
    "    version_2_with_negative：版本2中的一些问题没有答案。If true, the SQuAD examples contain some that do not have an answer.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                is_impossible = False\n",
    "                if is_training:\n",
    "                    if version_2_with_negative:\n",
    "                        is_impossible = qa[\"is_impossible\"]\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                        answer_length = len(orig_answer_text)\n",
    "                        start_position = char_to_word_offset[answer_offset]\n",
    "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "                        # Only add answers where the text can be exactly recovered from the document.\n",
    "                        # If this CAN'T happen it's likely due to weird Unicode stuff so we will just skip the example.\n",
    "                        # Note that this means for training mode, every example is NOT guaranteed to be preserved.\n",
    "                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                        # Runs basic whitespace cleaning and splitting on a piece of text.\n",
    "                        cleaned_answer_text = \" \".join(whitespace_tokenize(orig_answer_text))\n",
    "                        if actual_text.find(cleaned_answer_text) == -1:\n",
    "                            logger.warning(\"Could not find answer: '%s' vs. '%s'\",actual_text, cleaned_answer_text)\n",
    "                            continue\n",
    "                    else:\n",
    "                        start_position = -1\n",
    "                        end_position = -1\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=is_impossible)\n",
    "                examples.append(example)\n",
    "    \n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 cls_index,\n",
    "                 p_mask,\n",
    "                 paragraph_len,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.cls_index = cls_index\n",
    "        self.p_mask = p_mask\n",
    "        self.paragraph_len = paragraph_len\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "    \"\"\"Returns tokenized answer spans that better match the annotated answer. 处理答案，让答案与原文更匹配\"\"\"\n",
    "\n",
    "    # The SQuAD annotations are character based. We first project them to whitespace-tokenized words.\n",
    "    # But then after WordPiece tokenization, we can often find a \"better match\". For example:\n",
    "    #\n",
    "    #   Question: What year was John Smith born?\n",
    "    #   Context: The leader was John Smith (1895-1943).\n",
    "    #   Answer: 1895\n",
    "    #\n",
    "    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "    # the exact answer, 1895.\n",
    "    #\n",
    "    # However, this is not always possible. Consider the following:\n",
    "    #\n",
    "    #   Question: What country is the top exporter of electornics?\n",
    "    #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "    #   Answer: Japan\n",
    "    #\n",
    "    # In this case, the annotator chose \"Japan\" as a character sub-span of the word \"Japanese\".\n",
    "    # Since our WordPiece tokenizer does not split \"Japanese\", we just use \"Japanese\" as the annotation.\n",
    "    # This is fairly rare in SQuAD, but does happen.\n",
    "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "    for new_start in range(input_start, input_end + 1):\n",
    "        for new_end in range(input_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "            if text_span == tok_answer_text:\n",
    "                return (new_start, new_end)\n",
    "\n",
    "    return (input_start, input_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token. 选取一个答案在文本中间的段落\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 examples 转换为 feature 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=0, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s. 将样本转换为 feature \"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "    # cnt_pos, cnt_neg = 0, 0\n",
    "    # max_N, max_M = 1024, 1024\n",
    "    # f = np.zeros((max_N, max_M), dtype=np.float32)\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(examples):\n",
    "\n",
    "        # if example_index % 100 == 0:\n",
    "        #     logger.info('Converting %s/%s pos %s neg %s', example_index, len(examples), cnt_pos, cnt_neg)\n",
    "\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "        # 由于使用 word pisces tokenize\n",
    "        tok_to_orig_index = []  # tokenize 后当前 token 在 doc text 中的原始索引  [1, 2, 2, 3, 3, 4, ...]\n",
    "        orig_to_tok_index = []  # 原始 token 在 tokenize 后的索引节点 [0, 1, 3, 5, ...]\n",
    "        all_doc_tokens = []  # tokenize 后的token\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training and example.is_impossible:\n",
    "            tok_start_position = -1\n",
    "            tok_end_position = -1\n",
    "        if is_training and not example.is_impossible:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.  处理过长文本？\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name  命名 tuple\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []  # 用于保存 token 的列表 [CLS, query token, SEP, paragraph, SEP]\n",
    "            token_to_orig_map = {}  # 用于保存 token 与 doc word pisces token 索引映射\n",
    "            token_is_max_context = {}  # 是否是最合适的段落\n",
    "            segment_ids = []  # segment id [0, 0, 0, 1, 1, 1] 用于标记 segment\n",
    "            # p_mask 用于遮盖不用于答案的部分\n",
    "            # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "            # Original TF implem also keep the classification token (set to 0) (not sure why...)\n",
    "            p_mask = []\n",
    "\n",
    "            # CLS token at the beginning\n",
    "            if not cls_token_at_end:\n",
    "                tokens.append(cls_token)\n",
    "                segment_ids.append(cls_token_segment_id)\n",
    "                p_mask.append(0)\n",
    "                cls_index = 0\n",
    "\n",
    "            # Query  问题序列\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(sequence_a_segment_id)\n",
    "                p_mask.append(1)\n",
    "\n",
    "            # SEP token\n",
    "            tokens.append(sep_token)\n",
    "            segment_ids.append(sequence_a_segment_id)\n",
    "            p_mask.append(1)\n",
    "\n",
    "            # Paragraph\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(sequence_b_segment_id)\n",
    "                p_mask.append(0)\n",
    "            paragraph_len = doc_span.length\n",
    "\n",
    "            # SEP token\n",
    "            tokens.append(sep_token)\n",
    "            segment_ids.append(sequence_b_segment_id)\n",
    "            p_mask.append(1)\n",
    "\n",
    "            # CLS token at the end\n",
    "            if cls_token_at_end:\n",
    "                tokens.append(cls_token)\n",
    "                segment_ids.append(cls_token_segment_id)\n",
    "                p_mask.append(0)\n",
    "                cls_index = len(tokens) - 1  # Index of classification token\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.  模型输入的主要数据 4 个\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(pad_token)\n",
    "                input_mask.append(0 if mask_padding_with_zero else 1)\n",
    "                segment_ids.append(pad_token_segment_id)\n",
    "                p_mask.append(1)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            span_is_impossible = example.is_impossible\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training and not span_is_impossible:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                out_of_span = False\n",
    "                if not (tok_start_position >= doc_start and\n",
    "                        tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "                if out_of_span:\n",
    "                    start_position = 0\n",
    "                    end_position = 0\n",
    "                    span_is_impossible = True\n",
    "                else:\n",
    "                    doc_offset = len(query_tokens) + 2\n",
    "                    start_position = tok_start_position - doc_start + doc_offset\n",
    "                    end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if is_training and span_is_impossible:\n",
    "                start_position = cls_index\n",
    "                end_position = cls_index\n",
    "\n",
    "            if example_index < 20:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(tokens))\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                ]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training and span_is_impossible:\n",
    "                    logger.info(\"impossible example\")\n",
    "                if is_training and not span_is_impossible:\n",
    "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                    logger.info(\"start_position: %d\" % (start_position))\n",
    "                    logger.info(\"end_position: %d\" % (end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (answer_text))\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    cls_index=cls_index,\n",
    "                    p_mask=p_mask,\n",
    "                    paragraph_len=paragraph_len,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=span_is_impossible))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
    "    \"\"\"返回 TensorDataset \"\"\"\n",
    "    # Load data features from cache or dataset file  加载数据\n",
    "    input_file = args.predict_file if evaluate else args.train_file\n",
    "    cached_features_file = os.path.join(os.path.dirname(input_file), 'cached_{}_{}_{}'.format())\n",
    "    # 如果存在缓存文件，且不重新覆盖缓存，并且不需要输出样本，否者，重新读取数据构建样本\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache and not output_examples:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", input_file)\n",
    "        examples = read_squad_examples(input_file = input_file,\n",
    "                                       is_training = not evaluate,\n",
    "                                       version_2_with_negative = args.version_2_with_negative)\n",
    "        features = convert_examples_to_features(examples=examples,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                max_seq_length=args.max_seq_length,\n",
    "                                                doc_stride=args.doc_stride,\n",
    "                                                max_query_length=args.max_query_length,\n",
    "                                                is_training=not evaluate)\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
    "    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
    "    \n",
    "    if evaluate:\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                all_example_index, all_cls_index, all_p_mask)\n",
    "    else:\n",
    "        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                all_start_positions, all_end_positions,\n",
    "                                all_cls_index, all_p_mask)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试使用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
    "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "    # When we created the data, we kept track of the alignment between original\n",
    "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "    # now `orig_text` contains the span of our original text corresponding to the\n",
    "    # span that we predicted.\n",
    "    #\n",
    "    # However, `orig_text` may contain extra characters that we don't want in\n",
    "    # our prediction.\n",
    "    #\n",
    "    # For example, let's say:\n",
    "    #   pred_text = steve smith\n",
    "    #   orig_text = Steve Smith's\n",
    "    #\n",
    "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "    #\n",
    "    # We don't want to return `pred_text` because it's already been normalized\n",
    "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "    # our tokenizer does additional normalization like stripping accent\n",
    "    # characters).\n",
    "    #\n",
    "    # What we really want to return is \"Steve Smith\".\n",
    "    #\n",
    "    # Therefore, we have to apply a semi-complicated alignment heuristic between\n",
    "    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n",
    "    # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "    def _strip_spaces(text):\n",
    "        ns_chars = []\n",
    "        ns_to_s_map = collections.OrderedDict()\n",
    "        for (i, c) in enumerate(text):\n",
    "            if c == \" \":\n",
    "                continue\n",
    "            ns_to_s_map[len(ns_chars)] = i\n",
    "            ns_chars.append(c)\n",
    "        ns_text = \"\".join(ns_chars)\n",
    "        return (ns_text, ns_to_s_map)\n",
    "\n",
    "    # We first tokenize `orig_text`, strip whitespace from the result\n",
    "    # and `pred_text`, and check if they are the same length. If they are\n",
    "    # NOT the same length, the heuristic has failed. If they are the same\n",
    "    # length, we assume the characters are one-to-one aligned.\n",
    "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "\n",
    "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "    start_position = tok_text.find(pred_text)\n",
    "    if start_position == -1:\n",
    "        if verbose_logging:\n",
    "            logger.info(\n",
    "                \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "        return orig_text\n",
    "    end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "    if len(orig_ns_text) != len(tok_ns_text):\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
    "                        orig_ns_text, tok_ns_text)\n",
    "        return orig_text\n",
    "\n",
    "    # We then project the characters in `pred_text` back to `orig_text` using\n",
    "    # the character-to-character alignment.\n",
    "    tok_s_to_ns_map = {}\n",
    "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
    "        tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "    orig_start_position = None\n",
    "    if start_position in tok_s_to_ns_map:\n",
    "        ns_start_position = tok_s_to_ns_map[start_position]\n",
    "        if ns_start_position in orig_ns_to_s_map:\n",
    "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "    if orig_start_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map start position\")\n",
    "        return orig_text\n",
    "\n",
    "    orig_end_position = None\n",
    "    if end_position in tok_s_to_ns_map:\n",
    "        ns_end_position = tok_s_to_ns_map[end_position]\n",
    "        if ns_end_position in orig_ns_to_s_map:\n",
    "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "    if orig_end_position is None:\n",
    "        if verbose_logging:\n",
    "            logger.info(\"Couldn't map end position\")\n",
    "        return orig_text\n",
    "\n",
    "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
    "    return output_text\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list. 选取 logits 最高的前 n 个值 \"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_softmax(scores):\n",
    "    \"\"\"Compute softmax probability over raw logits. 计算 softmax \"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    max_score = None\n",
    "    for score in scores:\n",
    "        if max_score is None or score > max_score:\n",
    "            max_score = score\n",
    "\n",
    "    exp_scores = []\n",
    "    total_sum = 0.0\n",
    "    for score in scores:\n",
    "        x = math.exp(score - max_score)\n",
    "        exp_scores.append(x)\n",
    "        total_sum += x\n",
    "\n",
    "    probs = []\n",
    "    for score in exp_scores:\n",
    "        probs.append(score / total_sum)\n",
    "    return probs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file, verbose_logging,\n",
    "                      version_2_with_negative, null_score_diff_threshold):\n",
    "    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "    logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    scores_diff_json = collections.OrderedDict()\n",
    "    # 选取预测值\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        # keep track of the minimum score of null start+end of position 0\n",
    "        score_null = 1000000  # large and positive\n",
    "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
    "        null_start_logit = 0  # the start logit at the slice with min null score\n",
    "        null_end_logit = 0  # the end logit at the slice with min null score\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant？？\n",
    "            if version_2_with_negative:\n",
    "                feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "                if feature_null_score < score_null:\n",
    "                    score_null = feature_null_score\n",
    "                    min_null_feature_index = feature_index\n",
    "                    null_start_logit = result.start_logits[0]\n",
    "                    null_end_logit = result.end_logits[0]\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions,\n",
    "                    # e.g., predict that the start of the span is in the question.\n",
    "                    # We throw out all invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index]))\n",
    "        if version_2_with_negative:\n",
    "            prelim_predictions.append(\n",
    "                _PrelimPrediction(\n",
    "                    feature_index=min_null_feature_index,\n",
    "                    start_index=0,\n",
    "                    end_index=0,\n",
    "                    start_logit=null_start_logit,\n",
    "                    end_logit=null_end_logit))\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_logit + x.end_logit),\n",
    "            reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "            if pred.start_index > 0:  # this is a non-null prediction\n",
    "                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "                tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "                # De-tokenize WordPieces that have been split off.\n",
    "                tok_text = tok_text.replace(\" ##\", \"\")\n",
    "                tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "                # Clean whitespace\n",
    "                tok_text = tok_text.strip()\n",
    "                tok_text = \" \".join(tok_text.split())\n",
    "                orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
    "                if final_text in seen_predictions:\n",
    "                    continue\n",
    "\n",
    "                seen_predictions[final_text] = True\n",
    "            else:\n",
    "                final_text = \"\"\n",
    "                seen_predictions[final_text] = True\n",
    "\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit))\n",
    "        # if we didn't include the empty option in the n-best, include it\n",
    "        if version_2_with_negative:\n",
    "            if \"\" not in seen_predictions:\n",
    "                nbest.append(\n",
    "                    _NbestPrediction(\n",
    "                        text=\"\",\n",
    "                        start_logit=null_start_logit,\n",
    "                        end_logit=null_end_logit))\n",
    "                \n",
    "            # In very rare edge cases we could only have single null prediction.\n",
    "            # So we just create a nonce prediction in this case to avoid failure.\n",
    "            if len(nbest)==1:\n",
    "                nbest.insert(0,\n",
    "                    _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "        # 计算 softmax，根据 起始终止位置的概率\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "            if not best_non_null_entry:\n",
    "                if entry.text:\n",
    "                    best_non_null_entry = entry\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "        else:\n",
    "            # predict \"\" iff the null score - the score of best non-null > threshold？？？\n",
    "            score_diff = score_null - best_non_null_entry.start_logit - (\n",
    "                best_non_null_entry.end_logit)\n",
    "            scores_diff_json[example.qas_id] = score_diff\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example.qas_id] = \"\"\n",
    "            else:\n",
    "                all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    with open(output_prediction_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "    with open(output_nbest_file, \"w\") as writer:\n",
    "        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    if version_2_with_negative:\n",
    "        with open(output_null_log_odds_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试函数入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "    if not os.path.exists(args.output_dir) :\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    all_results = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=True):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1]\n",
    "                      }\n",
    "            if args.model_type != 'distilbert':\n",
    "                inputs['token_type_ids'] = None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids\n",
    "            example_indices = batch[3]\n",
    "            if args.model_type in ['xlnet', 'xlm']:  # 为什么不使用 p_mask\n",
    "                inputs.update({'cls_index': batch[4],\n",
    "                               'p_mask':    batch[5]})\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for i, example_index in enumerate(example_indices):\n",
    "            eval_feature = features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            \n",
    "            result = RawResult(unique_id    = unique_id,\n",
    "                               start_logits = to_list(outputs[0][i]),\n",
    "                               end_logits   = to_list(outputs[1][i]))\n",
    "            all_results.append(result)\n",
    "\n",
    "    # Compute predictions\n",
    "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "    if args.version_2_with_negative:\n",
    "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "    else:\n",
    "        output_null_log_odds_file = None\n",
    "\n",
    "    write_predictions(examples, features, all_results, args.n_best_size,\n",
    "                    args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                    output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                    args.version_2_with_negative, args.null_score_diff_threshold)\n",
    "\n",
    "    # Evaluate with the official SQuAD script\n",
    "    evaluate_options = EVAL_OPTS(data_file=args.predict_file,\n",
    "                                 pred_file=output_prediction_file,\n",
    "                                 na_prob_file=output_null_log_odds_file)\n",
    "    results = evaluate_on_squad(evaluate_options)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Setup logging \n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "    logger.info(\"evaluation parameters %s\", args)\n",
    "\n",
    "    # 初始化模型\n",
    "    args.model_type = args.model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "\n",
    "    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
    "    results = {}\n",
    "    if args.do_eval:\n",
    "        checkpoints = [args.output_dir]\n",
    "\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "\n",
    "        for checkpoint in checkpoints:\n",
    "            # Reload the model\n",
    "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "            model.to(args.device)\n",
    "\n",
    "            # Evaluate\n",
    "            result = evaluate(args, model, tokenizer, prefix=global_step)\n",
    "\n",
    "            result = dict((k + ('_{}'.format(global_step) if global_step else ''), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    logger.info(\"Results: {}\".format(results))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
