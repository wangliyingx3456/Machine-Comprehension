{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "SQuAD-BiDAF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR0RfoTYxu_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import logging\n",
        "import evaluate\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "# from time import gmtime, strftime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7phBfF6IxvAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKQVUSVcxvAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext import data, datasets\n",
        "from torchtext.vocab import GloVe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cItVWmKkxvAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "# Setup logging \n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxfeBoaWxvAK",
        "colab_type": "text"
      },
      "source": [
        "# 定义初始变量参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpuKwGoUxvAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 有关argparse的官方文档操作请查看：https://docs.python.org/3/library/argparse.html#module-argparse，\n",
        "# 关于parser.add_argument(）的详解请查看：https://blog.csdn.net/u013177568/article/details/62432761/\n",
        "# .parse_args()是将之前所有add_argument定义的参数在括号里进行赋值，没有赋值(args=[])，就返回参数各自default的默认值。默认情况下，中划线会转换为下划线.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgF1-kVAxvAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments(object):\n",
        "    # 模型参数\n",
        "    char_dim = 8\n",
        "    word_dim = 100\n",
        "    char_channel_size = 100\n",
        "    char_channel_width = 5\n",
        "    context_threshold = 400  # 文章的长度\n",
        "    hidden_size = 100\n",
        "    dropout = 0.2\n",
        "    # 训练参数\n",
        "    epoch = 2\n",
        "    learning_rate = 0.5\n",
        "    exp_decay_rate = 0.999\n",
        "    logging_steps = 100\n",
        "    prediction_file = './data/prediction.json'  # 预测文件\n",
        "    model_dir = 'BiDAF.pkl' # 模型保存路径\n",
        "    \n",
        "    # 数据参数\n",
        "    dev_batch_size = 32\n",
        "    train_batch_size = 32\n",
        "    train_file = './data/train-v1.1.json'  # 原始训练集\n",
        "    dev_file = './data/dev-v1.1.json'  # 原始验证集\n",
        "    train_torchtext_file = './data/train_torchtext.json'  # 能够被 torchtext 处理的训练集\n",
        "    dev_torchtext_file = './data/dev_torchtext.json'  # 能够被 TorchText 处理的验证集\n",
        "    train_example_file = './data/train_examples.pt'  # torchtext example 数据 \n",
        "    dev_example_file = './data/dev_examples.pt'  # torchtext example 数据 \n",
        "    overwrite_cache = False\n",
        "    dataset_file = './data/dev-v1.1.json'  # 用于测试\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNLpmEZ7xvAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Arguments()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x8GKKWwxvAR",
        "colab_type": "text"
      },
      "source": [
        "# 二、SQuAD问答数据预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw0Vgve6xvAS",
        "colab_type": "text"
      },
      "source": [
        "## 查看数据集结构"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw9au1MBxvBa",
        "colab_type": "text"
      },
      "source": [
        "SQuAD 数据格式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etuMublKxvAU",
        "colab_type": "text"
      },
      "source": [
        "SQuAD 数据集的格式是字典格式，包含 data 和 version 两个键。其中 data 中保存了数据集内容，version 用于记录当前数据集的版本号。data 的值是一个列表，列表的元素为某一主题的文章集合，格式为字典格式。该主题文章集合字典包含两个键 title 和 paragraphs。其中 title 为主题名，paragraphs 为文章的列表。文章也是字典格式，包含两个键 context 和 qas，分别表示文章及文章的问题答案。其中文章为字符串，文章的问题及答案用列表保存。每个问题及对应的答案使用字典保存，键包括 answers, question, id 分别用于保存问题、答案及其唯一标识。答案有多个，使用列表保存，每一个答案为一个字典保存了答案的起始位置及其文本内容。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Tem05OxvBb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```json\n",
        "// 数据架构如下\n",
        "{\n",
        "    \"data\": [\n",
        "        {\n",
        "            \"title\": \"Super_Bowl_50\", // 第一个主题\n",
        "            \"paragraphs\": [\n",
        "                {\n",
        "                    \"context\": \" numerals 50.......\", // 每个主题会有很多context短文,这里只列出一个\n",
        "                    \"qas\": [  // 这个列表里放问题和答案的位置，每篇context会有很有很多answer和question，这里只列出一个\n",
        "                        {\n",
        "                            \"answers\": [  // 一个问题会有三个答案，三个答案都是对的，只是在context不同或相同位置\n",
        "                                {         // 下面三个答案都在相同的位置\n",
        "                                    \"answer_start\": 177,  // 答案在文中的起始位置是第177的字符。\n",
        "                                    \"text\": \"Denver Broncos\"\n",
        "                                },\n",
        "                                {\n",
        "                                    \"answer_start\": 177,\n",
        "                                    \"text\": \"Denver Broncos\"\n",
        "                                },\n",
        "                                {\n",
        "                                    \"answer_start\": 177,\n",
        "                                    \"text\": \"Denver Broncos\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"question\": \"Which NFL team represented the AFC at Super Bowl 50?\",\n",
        "                            \"id\": \"56be4db0acb8001400a502ec\"\n",
        "                        }\n",
        "\n",
        "                    ]\n",
        "                }\n",
        "                \n",
        "            ]\n",
        "        },\n",
        "        \n",
        "        {\n",
        "            \"title\": \"Warsaw\", // 第二个主题\n",
        "            \"paragraphs\":   \n",
        "        },\n",
        "        \n",
        "        {\n",
        "            \"title\": \"Normans\", // 第三个主题\n",
        "            \"paragraphs\": \n",
        "        },\n",
        "        \n",
        "        {\n",
        "            \"title\": \"Nikola_Tesla\", // 第四个主题\n",
        "            \"paragraphs\": \n",
        "        },\n",
        "        ........... // 还有很多\n",
        "        \n",
        "    ],\n",
        "    \"version\": \"1.1\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmp1peA6xvAW",
        "colab_type": "text"
      },
      "source": [
        "## 加载数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC1hiu8jxvAX",
        "colab_type": "text"
      },
      "source": [
        "定义分词方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNGil2KVzxUj",
        "colab_type": "code",
        "outputId": "bcd180c5-6577-4fe8-ddc7-2b6c9c3e3c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "nltk.download('punkt')  # 下载 token"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAzkG8KZxvAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(tokens):\n",
        "    # 输入字符串，分割成单词列表\n",
        "    # nltk.word_tokenize(tokens)分词，replace规范化引号，方便后面处理\n",
        "    tokens = [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
        "    \n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soMSx14DxvAZ",
        "colab_type": "text"
      },
      "source": [
        "将原始 json 转换为 TorchText 能够处理的 json 格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1UJO-gCxvAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_raw_json(path):\n",
        "    examples = []\n",
        "    # 需要去除的字符\n",
        "    abnormals = [' ', '\\n', '\\u3000', '\\u202f', '\\u2009']\n",
        "    \n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "        data = data['data']  # 返回值 data 是个列表，列表的元素为字典\n",
        "\n",
        "    for articles in tqdm(data, desc='Processing'):  # 处理 data 中每个主题\n",
        "        for paragraph in articles['paragraphs']:  # 处理主题中的每个 paragraph\n",
        "            # 每个 paragraph 包含 context 和 qas\n",
        "            context = paragraph['context']  # 段落文本\n",
        "            tokens = word_tokenize(context)\n",
        "            for qa in paragraph['qas']:  # 包含多个问题及答案\n",
        "                # 每个 qa 包含 answers 和 question 及唯一标识符 id\n",
        "                qa_id = qa['id']\n",
        "                question = qa['question']\n",
        "                for ans in qa['answers']:  # 处理每个答案\n",
        "                    # 每个 answer 包含 text 及 answer_start\n",
        "                    answer = ans['text']\n",
        "                    s_idx = ans['answer_start']\n",
        "                    e_idx = s_idx + len(answer)\n",
        "                    \n",
        "                    # 重新计算答案的起始位置，使用字符计算位置改为使用单词计算位置\n",
        "                    l = 0\n",
        "                    s_found = False\n",
        "                    for i, t in enumerate(tokens):\n",
        "                        # 在 context 中的一个非空白符的起始位置\n",
        "                        while l < len(context):\n",
        "                            if context[l] in abnormals:\n",
        "                                l += 1\n",
        "                            else:\n",
        "                                break\n",
        "                        # exceptional cases  word_tokenize 函数替换的引号\n",
        "                        if t[0] == '\"' and context[l:l + 2] == '\\'\\'':\n",
        "                            t = '\\'\\'' + t[1:]\n",
        "                        elif t == '\"' and context[l:l + 2] == '\\'\\'':\n",
        "                            t = '\\'\\''\n",
        "                        # 记录起始位置及终止位置\n",
        "                        l += len(t)\n",
        "                        if l > s_idx and s_found == False:\n",
        "                            s_idx = i\n",
        "                            s_found = True\n",
        "                        if l >= e_idx:\n",
        "                            e_idx = i\n",
        "                            break\n",
        "                    # 保存一个样本\n",
        "                    examples.append(dict([('id', qa_id),\n",
        "                                      ('context', context),\n",
        "                                      ('question', question),\n",
        "                                      ('answer', answer),\n",
        "                                      ('s_idx', s_idx),\n",
        "                                      ('e_idx', e_idx)]))\n",
        "    \n",
        "    save_path = args.train_torchtext_file if path == args.train_file else args.dev_torchtext_file\n",
        "    logger.info(\"Save json data to new file %s\", save_path)\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        for example in examples:\n",
        "            json.dump(example, f)\n",
        "            print('', file=f)  # 换行\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEq7klFUxvAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SQuADProcessor(object):\n",
        "    \"\"\" 处理 SQuAD 原始数据 \"\"\"\n",
        "    def __init__(self, args):\n",
        "        # 是否存在处理好的 TorchText json 文件\n",
        "        if os.path.exists(args.train_torchtext_file) and os.path.exists(args.dev_torchtext_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading data from processed file %s and %s\", args.train_torchtext_file, args.dev_torchtext_file)\n",
        "        else:\n",
        "            logger.info(\"Preprocessing json data from dataset file at %s and %s\", args.dev_file, args.train_file)\n",
        "            preprocess_raw_json(args.train_file)\n",
        "            preprocess_raw_json(args.dev_file)\n",
        "        \n",
        "        # 用torchtext处理数据\n",
        "        self.RAW = data.RawField()  # 这个是完全空白的field，意味着不经过任何处理\n",
        "        # explicit declaration for torchtext compatibility\n",
        "        self.RAW.is_target = False\n",
        "        self.CHAR_NESTING = data.Field(batch_first=True, tokenize=list, lower=True)\n",
        "        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)\n",
        "        self.WORD = data.Field(batch_first=True, tokenize=word_tokenize, lower=True, include_lengths=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None, use_vocab=False)\n",
        "        # 数据中包含的数据\n",
        "        dict_fields = {'id': ('id', self.RAW),\n",
        "                       's_idx': ('s_idx', self.LABEL),\n",
        "                       'e_idx': ('e_idx', self.LABEL),\n",
        "                       'context': [('c_word', self.WORD), ('c_char', self.CHAR)],\n",
        "                       'question': [('q_word', self.WORD), ('q_char', self.CHAR)]}\n",
        "\n",
        "        list_fields = [('id', self.RAW), ('s_idx', self.LABEL), ('e_idx', self.LABEL),\n",
        "                       ('c_word', self.WORD), ('c_char', self.CHAR),\n",
        "                       ('q_word', self.WORD), ('q_char', self.CHAR)]\n",
        "        \n",
        "        # 判断 是否有 torchtext example 数据 \n",
        "        if os.path.exists(args.train_example_file) and os.path.exists(args.dev_example_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading data from cached file %s and %s\", args.train_example_file, args.dev_example_file)\n",
        "            train_examples = torch.load(os.path.join(args.train_example_file))\n",
        "            dev_examples = torch.load(os.path.join(args.dev_example_file))\n",
        "\n",
        "            self.train = data.Dataset(examples=train_examples, fields=list_fields)\n",
        "            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)\n",
        "        else:\n",
        "            logger.info(\"Loading data from processed torchtext json file %s and %s\", \n",
        "                        args.train_torchtext_file, args.dev_torchtext_file)\n",
        "             # 创建训练集和验证集\n",
        "            self.train, self.dev = data.TabularDataset.splits(\n",
        "                path='',\n",
        "                train=args.train_torchtext_file,\n",
        "                validation=args.dev_torchtext_file,\n",
        "                format='json',\n",
        "                fields=dict_fields)\n",
        "            # 保存处理后的 torchtext example 数据 \n",
        "            torch.save(self.train.examples, args.train_example_file)\n",
        "            torch.save(self.dev.examples, args.dev_example_file)\n",
        "        logger.info(\"Nmber of train examples %d\", len(self.train))\n",
        "        #cut too long context in the training set for efficiency.  改进，将长的文章截断！\n",
        "        if args.context_threshold > 0:\n",
        "            self.train.examples = [e for e in self.train.examples if len(e.c_word) <= args.context_threshold]\n",
        "        logger.info(\"Nmber of train examples %d\", len(self.train))\n",
        "        logger.info(\"Building vacab...\")\n",
        "        self.CHAR.build_vocab(self.train, self.dev) # 字符向量没有设置vector\n",
        "        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name='6B', dim=args.word_dim))\n",
        "        # 加载Glove向量，args.word_dim = 100\n",
        "\n",
        "        logger.info(\"Building iterators...\")\n",
        "        # 生成迭代器\n",
        "        self.train_iter, self.dev_iter = \\\n",
        "            data.BucketIterator.splits((self.train, self.dev),\n",
        "                                       batch_sizes=[args.train_batch_size, args.dev_batch_size],\n",
        "                                       device=args.device,\n",
        "                                       sort_key=lambda x: len(x.c_word))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRLrh7I33IqV",
        "colab_type": "text"
      },
      "source": [
        "**注意！内存消耗特别高！可能需要16G的内存**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE9IsIBDxvAg",
        "colab_type": "code",
        "outputId": "80e215cb-6933-4190-bf71-be582f308bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "data = SQuADProcessor(args)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/28/2019 01:42:52 - INFO - __main__ -   Preprocessing json data from dataset file at ./data/dev-v1.1.json and ./data/train-v1.1.json\n",
            "Processing: 100%|██████████| 442/442 [00:21<00:00, 20.27it/s]\n",
            "10/28/2019 01:43:14 - INFO - __main__ -   Save json data to new file ./data/train_torchtext.json\n",
            "Processing: 100%|██████████| 48/48 [00:03<00:00, 13.91it/s]\n",
            "10/28/2019 01:43:19 - INFO - __main__ -   Save json data to new file ./data/dev_torchtext.json\n",
            "10/28/2019 01:43:20 - INFO - __main__ -   Loading data from processed torchtext json file ./data/train_torchtext.json and ./data/dev_torchtext.json\n",
            "10/28/2019 01:51:49 - INFO - __main__ -   Nmber of train examples 87599\n",
            "10/28/2019 01:51:49 - INFO - __main__ -   Nmber of train examples 87416\n",
            "10/28/2019 01:51:49 - INFO - __main__ -   Building vacab...\n",
            "10/28/2019 01:52:36 - INFO - torchtext.vocab -   Downloading vectors from http://nlp.stanford.edu/data/glove.6B.zip\n",
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
            "10/28/2019 01:59:05 - INFO - torchtext.vocab -   Extracting vectors into .vector_cache\n",
            "10/28/2019 01:59:23 - INFO - torchtext.vocab -   Loading vectors from .vector_cache/glove.6B.100d.txt\n",
            "100%|█████████▉| 399863/400000 [00:19<00:00, 21177.72it/s]10/28/2019 01:59:43 - INFO - torchtext.vocab -   Saving vectors to .vector_cache/glove.6B.100d.txt.pt\n",
            "10/28/2019 01:59:49 - INFO - __main__ -   Building iterators...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea_x-bYYxvA4",
        "colab_type": "code",
        "outputId": "2a414a67-637b-4fe5-9db9-590b23a518fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 下面为args新增参数，并赋值\n",
        "# hasattr() getattr() setattr() 函数使用方法详解https://www.cnblogs.com/cenyu/p/5713686.html\n",
        "setattr(args, 'char_vocab_size', len(data.CHAR.vocab)) # 设置属性args.char_vocab_size的值 = len(data.CHAR.vocab)\n",
        "setattr(args, 'word_vocab_size', len(data.WORD.vocab))\n",
        "setattr(args, 'dataset_file', args.dev_file)\n",
        "# setattr(args, 'prediction_file', f'prediction{args.gpu}.out')\n",
        "print('data loading complete!')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data loading complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR2DBjcRxvAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "34ce6c8d-8098-433c-9dec-fdd274230719"
      },
      "source": [
        "batch = next(iter(data.train_iter)) #一个batch的信息\n",
        "print(batch)\n",
        "# 训练集的batch_sizes=32\n",
        "# batch.c_word = 32x264， 264 是32个样本中最长样本token的单词数\n",
        "# batch.c_char = 32x264x25， 18 是某个单词字符的最大的数量"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399863/400000 [00:30<00:00, 21177.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 32]\n",
            "\t[.id]:['5733b0fb4776f41900661045', '57282d193acd2414000df653', '570713c490286e26004fc8ca', '57297608af94a219006aa476', '57261b3b38643c19005ad007', '56d3661659d6e414001462d8', '57265c18dd62a815002e82ab', '5731e68eb9d445190005e641', '5726d168f1498d1400e8ec36', '56e82b5c37bdd419002c4480', '5733b425d058e614000b60bd', '5726cff25951b619008f7eb0', '57318491e6313a140071d003', '5728233a2ca10214002d9ead', '573610e56c16ec1900b9295a', '5732c3521d5d2e14009ff8a2', '57302525a23a5019007fce5b', '56dfc11d231d4119001abda7', '56f7f512aef2371900625cd5', '5726609b708984140094c414', '5726f2815951b619008f831d', '5706068852bb8914006897bf', '56e7825400c9c71400d771ea', '5719d8214faf5e1900b8a81c', '57098332ed30961900e84258', '56e8862699e8941900975e58', '56dfa7277aa994140058dfa2', '57291d851d0469140077906b', '573267bee17f3d1400422950', '572fa92fb2c2fd14005682d3', '572833943acd2414000df6d4', '56f9888b9e9bad19000a0a4e']\n",
            "\t[.s_idx]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
            "\t[.e_idx]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
            "\t[.c_word]:('[torch.cuda.LongTensor of size 32x284 (GPU 0)]', '[torch.cuda.LongTensor of size 32 (GPU 0)]')\n",
            "\t[.c_char]:[torch.cuda.LongTensor of size 32x284x27 (GPU 0)]\n",
            "\t[.q_word]:('[torch.cuda.LongTensor of size 32x23 (GPU 0)]', '[torch.cuda.LongTensor of size 32 (GPU 0)]')\n",
            "\t[.q_char]:[torch.cuda.LongTensor of size 32x23x15 (GPU 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uZfGPhmxvA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "4c7f49d7-c99a-450f-8b94-825b3cd675b8"
      },
      "source": [
        "print(len(data.WORD.vocab)) # 108777个单词\n",
        "print(data.WORD.vocab.vectors.shape) # 词向量维度\n",
        "\n",
        "print(data.WORD.vocab.itos[:50]) # 前50个词频最高的单词\n",
        "print(\"------\"*10)\n",
        "print(list(data.WORD.vocab.stoi.items())[0:50]) # 对应的索引"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108777\n",
            "torch.Size([108777, 100])\n",
            "['<unk>', '<pad>', 'the', ',', 'of', '.', 'and', 'in', 'to', 'a', '\"', 'is', 'was', 'as', ')', '(', 'for', '?', 'by', 'that', 'with', \"'s\", 'on', 'from', 'are', 'what', 'which', 'it', 'were', 'at', 'an', 'or', 'be', 'this', 'his', 'have', 'not', 'their', 'also', 'has', 'its', 'who', 'had', 'he', ';', 'other', 'first', 'one', 'but', 'new']\n",
            "------------------------------------------------------------\n",
            "[('<unk>', 0), ('<pad>', 1), ('the', 2), (',', 3), ('of', 4), ('.', 5), ('and', 6), ('in', 7), ('to', 8), ('a', 9), ('\"', 10), ('is', 11), ('was', 12), ('as', 13), (')', 14), ('(', 15), ('for', 16), ('?', 17), ('by', 18), ('that', 19), ('with', 20), (\"'s\", 21), ('on', 22), ('from', 23), ('are', 24), ('what', 25), ('which', 26), ('it', 27), ('were', 28), ('at', 29), ('an', 30), ('or', 31), ('be', 32), ('this', 33), ('his', 34), ('have', 35), ('not', 36), ('their', 37), ('also', 38), ('has', 39), ('its', 40), ('who', 41), ('had', 42), ('he', 43), (';', 44), ('other', 45), ('first', 46), ('one', 47), ('but', 48), ('new', 49)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6UWz3vvxvA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "62251ce5-458d-47d9-e384-3ad57bb637f9"
      },
      "source": [
        "print(len(data.CHAR.vocab)) # 1307个单词\n",
        "print(data.CHAR.vocab.itos[:50]) # 108777个单词\n",
        "print(\"------\"*10)\n",
        "print(list(data.CHAR.vocab.stoi.items())[0:50]) # 对应的索引"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1307\n",
            "['<unk>', '<pad>', 'e', 't', 'a', 'i', 'n', 'o', 's', 'r', 'h', 'l', 'd', 'c', 'u', 'm', 'f', 'p', 'g', 'w', 'y', 'b', ',', 'v', '.', 'k', '1', '0', 'x', '2', '\"', '-', 'j', '9', \"'\", ')', '(', '?', 'z', '5', '8', 'q', '3', '4', '7', '6', ';', ':', '–', '%']\n",
            "------------------------------------------------------------\n",
            "[('<unk>', 0), ('<pad>', 1), ('e', 2), ('t', 3), ('a', 4), ('i', 5), ('n', 6), ('o', 7), ('s', 8), ('r', 9), ('h', 10), ('l', 11), ('d', 12), ('c', 13), ('u', 14), ('m', 15), ('f', 16), ('p', 17), ('g', 18), ('w', 19), ('y', 20), ('b', 21), (',', 22), ('v', 23), ('.', 24), ('k', 25), ('1', 26), ('0', 27), ('x', 28), ('2', 29), ('\"', 30), ('-', 31), ('j', 32), ('9', 33), (\"'\", 34), (')', 35), ('(', 36), ('?', 37), ('z', 38), ('5', 39), ('8', 40), ('q', 41), ('3', 42), ('4', 43), ('7', 44), ('6', 45), (';', 46), (':', 47), ('–', 48), ('%', 49)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hElASVraxvBG",
        "colab_type": "text"
      },
      "source": [
        "## BIDAF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ecf5WgQxvBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\" LSTM 层 \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False, num_layers=1, bidirectional=False, dropout=0.2):\n",
        "        \"\"\"\n",
        "        input_size: hidden_size * 2，由于输入为 word embedding 和 character embedding 的 concatenation。\n",
        "        hidden_size: LSTM 的隐藏层维度。\n",
        "        bidirectional: 是否使用双向LSTM。\n",
        "        batch_first: batch size 维度是否在第一个维度。\n",
        "        dropout: 默认为 0.2。\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size=input_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           num_layers=num_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=batch_first)\n",
        "        self.reset_params()  # 重置参数\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def reset_params(self):\n",
        "        \"\"\" 初始化 LSTM 参数 \"\"\"\n",
        "        for i in range(self.rnn.num_layers):\n",
        "            nn.init.orthogonal_(getattr(self.rnn, f'weight_hh_l{i}')) # hidden-hidden weights\n",
        "            # weight_hh_l{i}、weight_ih_l{i}、bias_hh_l{i}、bias_ih_l{i} 都是nn.LSTM源码里的参数\n",
        "            # getattr取出源码里参数的值，用nn.init.orthogonal_正交进行重新初始化\n",
        "            # nn.init初始化方法看这个链接：https://www.aiuai.cn/aifarm613.html\n",
        "            nn.init.kaiming_normal_(getattr(self.rnn, f'weight_ih_l{i}')) # input-hidden weights\n",
        "            nn.init.constant_(getattr(self.rnn, f'bias_hh_l{i}'), val=0) # hidden-hidden bias\n",
        "            nn.init.constant_(getattr(self.rnn, f'bias_ih_l{i}'), val=0) # input-hidden bias\n",
        "            getattr(self.rnn, f'bias_hh_l{i}').chunk(4)[1].fill_(1)\n",
        "            # .chunk看下这个链接：https://blog.csdn.net/XuM222222/article/details/92380538\n",
        "            # .fill_(1),下划线代表直接替换，看链接：https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html\n",
        "\n",
        "            if self.rnn.bidirectional: # 双向，需要初始化反向的参数\n",
        "                nn.init.orthogonal_(getattr(self.rnn, f'weight_hh_l{i}_reverse'))\n",
        "                nn.init.kaiming_normal_(getattr(self.rnn, f'weight_ih_l{i}_reverse'))\n",
        "                nn.init.constant_(getattr(self.rnn, f'bias_hh_l{i}_reverse'), val=0)\n",
        "                nn.init.constant_(getattr(self.rnn, f'bias_ih_l{i}_reverse'), val=0)\n",
        "                getattr(self.rnn, f'bias_hh_l{i}_reverse').chunk(4)[1].fill_(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: 包含word embedding 和 character embedding 的 concatenation 和 序列长度的列表\n",
        "        return\n",
        "        x：LSTM 每个时间步的隐藏状态。\n",
        "        h：最后一个时间步的隐藏状态。\n",
        "        \"\"\"\n",
        "        # x 是一个元组(c, c_lens)\n",
        "        x, x_len = x\n",
        "        # x = (batch, seq_len, hidden_size * 2)\n",
        "        # x_len = (batch) 一个batch中所有context或question的样本长度\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # 看下这篇博客理解：https://www.cnblogs.com/sbj123456789/p/9834018.html\n",
        "        x_len_sorted, x_idx = torch.sort(x_len, descending=True)\n",
        "        x_sorted = x.index_select(dim=0, index=x_idx)\n",
        "        _, x_ori_idx = torch.sort(x_idx)\n",
        "\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=True)\n",
        "        x_packed, (h, c) = self.rnn(x_packed)\n",
        "        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)[0]\n",
        "        \n",
        "        x = x.index_select(dim=0, index=x_ori_idx)\n",
        "        h = h.permute(1, 0, 2).contiguous().view(-1, h.size(0) * h.size(2)).squeeze()\n",
        "        h = h.index_select(dim=0, index=x_ori_idx)\n",
        "        # x = (batch, seq_len, hidden_size * 2) \n",
        "        # h = (1, batch, hidden_size * 2) 这个维度不用管\n",
        "        return x, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpVy0LaBxvBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(nn.Module):\n",
        "    \"\"\" 一个线性层 \"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(Linear, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        # in_features = hidden_size * 2\n",
        "        # out_features = hidden_size * 2\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        self.reset_params()\n",
        "\n",
        "    def reset_params(self):\n",
        "        \"\"\" 初始化参数 \"\"\"\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        nn.init.constant_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'dropout'): # 判断self有没有'dropout'这个参数，返回bool值\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbV8ReooxvBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 看英文论文或这篇博客理解模型：https://blog.csdn.net/u014665013/article/details/79793395\n",
        "class BiDAF(nn.Module):\n",
        "    def __init__(self, args, pretrained):\n",
        "        # pretrained = data.WORD.vocab.vectors = (108777, 100)\n",
        "        super(BiDAF, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        # 1. Character Embedding Layer \n",
        "        # 字符编码层 char_vocab_size = 1307，char_dim = 8\n",
        "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=1)\n",
        "        # 初始化权重 \n",
        "        nn.init.uniform_(self.char_emb.weight, -0.001, 0.001)\n",
        "        # Char-CNN 用于提取 charactor embedding 特征\n",
        "        # char_channel_size = 100 卷积核数量，卷积核维度 [char_dim, char_channel_width] = [8,5]\n",
        "        self.char_conv = nn.Conv2d(1, args.char_channel_size, (args.char_dim, args.char_channel_width))\n",
        "        \n",
        "        # 2. Word Embedding Layer\n",
        "        # initialize word embedding with GloVe ，使用 Glove 向量初始化词向量权重\n",
        "        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=True)\n",
        "\n",
        "        assert self.args.hidden_size * 2 == (self.args.char_channel_size + self.args.word_dim)\n",
        "        \n",
        "        # highway network\n",
        "        for i in range(2):\n",
        "            # 设置 highway_linear 和 highway_gate，hidden_size = 100\n",
        "            setattr(self, f'highway_linear{i}',\n",
        "                    nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2), nn.ReLU()))\n",
        "            setattr(self, f'highway_gate{i}',\n",
        "                    nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2), nn.Sigmoid()))\n",
        "\n",
        "        # 3. Contextual Embedding Layer\n",
        "        # 上下文，和答案嵌入层，用的LSTM\n",
        "        # 下面LSTM定位到了自定义的class LSTM(nn.Module)。\n",
        "        self.context_LSTM = LSTM(input_size=args.hidden_size * 2,\n",
        "                                 hidden_size=args.hidden_size,\n",
        "                                 bidirectional=True,\n",
        "                                 batch_first=True,\n",
        "                                 dropout=args.dropout) \n",
        "\n",
        "        # 4. Attention Flow Layer\n",
        "        self.att_weight_c = Linear(args.hidden_size * 2, 1)\n",
        "        self.att_weight_q = Linear(args.hidden_size * 2, 1)\n",
        "        self.att_weight_cq = Linear(args.hidden_size * 2, 1)\n",
        "\n",
        "        # 5. Modeling Layer\n",
        "        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * 8,\n",
        "                                   hidden_size=args.hidden_size,\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=args.dropout)\n",
        "\n",
        "        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * 2,\n",
        "                                   hidden_size=args.hidden_size,\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=args.dropout)\n",
        "\n",
        "        # 6. Output Layer\n",
        "        self.p1_weight_g = Linear(args.hidden_size * 8, 1, dropout=args.dropout)\n",
        "        self.p1_weight_m = Linear(args.hidden_size * 2, 1, dropout=args.dropout)\n",
        "        self.p2_weight_g = Linear(args.hidden_size * 8, 1, dropout=args.dropout)\n",
        "        self.p2_weight_m = Linear(args.hidden_size * 2, 1, dropout=args.dropout)\n",
        "\n",
        "        self.output_LSTM = LSTM(input_size=args.hidden_size * 2,\n",
        "                                hidden_size=args.hidden_size,\n",
        "                                bidirectional=True,\n",
        "                                batch_first=True,\n",
        "                                dropout=args.dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # batch里面有'id','s_idx','e_idx', 'c_word','c_char','q_word', 'q_char'数据\n",
        "        # TODO: More memory-efficient architecture\n",
        "        def char_emb_layer(x):\n",
        "            \"\"\"\n",
        "            :param x: (batch, seq_len, word_len)\n",
        "            :return: (batch, seq_len, char_channel_size)\n",
        "            \"\"\"\n",
        "            # x = [batch_sizes,seq_len,word_len]\n",
        "            batch_size = x.size(0)\n",
        "            # [batch, seq_len, word_len, char_dim]\n",
        "            x = self.dropout(self.char_emb(x))\n",
        "            # [batch * seq_len, 1, char_dim, word_len], 增加 channel 的维度\n",
        "            x = x.view(-1, self.args.char_dim, x.size(2)).unsqueeze(1)\n",
        "            # -> [batch*seq_len, char_channel_size, 1, conv_len] -> [batch*seq_len, char_channel_size, conv_len]\n",
        "            x = self.char_conv(x).squeeze()\n",
        "            # -> [batch*seq_len, char_channel_size, 1] -> [batch*seq_len, char_channel_size]\n",
        "            x = F.max_pool1d(x, x.size(2)).squeeze()\n",
        "            # [batch, seq_len, char_channel_size]\n",
        "            x = x.view(batch_size, -1, self.args.char_channel_size)\n",
        "\n",
        "            return x\n",
        "\n",
        "        def highway_network(x1, x2):\n",
        "            \"\"\"\n",
        "            :param x1: char embedding [batch, seq_len, char_channel_size]\n",
        "            :param x2: word embedding [batch, seq_len, word_dim]\n",
        "            :return: [batch, seq_len, hidden_size * 2]\n",
        "            \"\"\"\n",
        "            # 拼接 char embed 与 word embed: [batch, seq_len, char_channel_size+word_dim]=hidden_size*2\n",
        "            x = torch.cat([x1, x2], dim=-1)\n",
        "            for i in range(2):\n",
        "                # h: [batch, seq_len, hidden_size*2]\n",
        "                h = getattr(self, f'highway_linear{i}')(x)\n",
        "                # g: [batch, seq_len, hidden_size*2]\n",
        "                g = getattr(self, f'highway_gate{i}')(x)\n",
        "                x = g * h + (1 - g) * x\n",
        "            # (batch, seq_len, hidden_size * 2)\n",
        "            return x\n",
        "\n",
        "        def att_flow_layer(c, q):\n",
        "            \"\"\"\n",
        "            :param c: 文章的 LSTM 输出 [batch, c_len, hidden_size*2]\n",
        "            :param q: 问题的 LSTM 输出 [batch, q_len, hidden_size*2]\n",
        "            :return: [batch, c_len, q_len]\n",
        "            \"\"\"\n",
        "            c_len = c.size(1)\n",
        "            q_len = q.size(1)\n",
        "\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #cq_tiled = c_tiled * q_tiled\n",
        "            #cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "\n",
        "            cq = []\n",
        "            # 1、相似度计算方式，看下这篇博客理解：https://blog.csdn.net/u014665013/article/details/79793395\n",
        "            for i in range(q_len):\n",
        "                # [batch, 1, hidden_size*2]  .select：https://blog.csdn.net/hungryof/article/details/51802829\n",
        "                qi = q.select(1, i).unsqueeze(1)\n",
        "                # -> [batch, c_len, hidden_size*2] -> [batch, c_len, 1] -> [batch, c_len]\n",
        "                ci = self.att_weight_cq(c * qi).squeeze()\n",
        "                cq.append(ci)\n",
        "            # [batch, c_len, q_len]\n",
        "            cq = torch.stack(cq, dim=-1)\n",
        "            \n",
        "            # [batch, c_len, hidden_size*2] -> [batch, c_len, 1] -> [batch, c_len, q_len]\n",
        "            # [batch, c_len, hidden_size*2] -> [batch, q_len, 1] -> [batch, c_len, q_len]\n",
        "            # [batch, c_len, q_len]\n",
        "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
        "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + cq\n",
        "            \n",
        "            # 2、context-to-query attention(C2Q): 计算对每一个 context word 而言哪些 query words 和它最相关。\n",
        "            # [batch, c_len, q_len]\n",
        "            a = F.softmax(s, dim=2)\n",
        "            # [batch, c_len, q_len] dot [batch, q_len, hidden_size*2] -> [batch, c_len, hidden_size*2]\n",
        "            c2q_att = torch.bmm(a, q)\n",
        "            \n",
        "            # 3、query-to-context attention(Q2C): 计算对每一个 query word 而言哪些 context words 和它最相关\n",
        "            # [batch, c_len, q_len] -> [batch, c_len] -> [batch, 1, c_len]\n",
        "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
        "            # [batch, 1, c_len] dot [batch, c_len, hidden_size * 2] -> [batch, hidden_size * 2]\n",
        "            q2c_att = torch.bmm(b, c).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2) (tiled)\n",
        "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
        "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
        "            \n",
        "            # 4、最后将context embedding和C2Q、Q2C的结果（三个矩阵）拼接起来\n",
        "            # (batch, c_len, hidden_size * 8)\n",
        "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
        "            \n",
        "            return x\n",
        "\n",
        "        def output_layer(g, m, l):\n",
        "            \"\"\"\n",
        "            :param g: (batch, c_len, hidden_size * 8)\n",
        "            :param m: (batch, c_len ,hidden_size * 2)\n",
        "             #  l = c_lens\n",
        "            :return: p1: (batch, c_len), p2: (batch, c_len)\n",
        "            \"\"\"\n",
        "            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()\n",
        "            # (batch, c_len)\n",
        "            m2 = self.output_LSTM((m, l))[0]\n",
        "            # (batch, c_len, hidden_size * 2)\n",
        "            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()\n",
        "            # (batch, c_len)\n",
        "            return p1, p2\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        # 令:一个batch中单词数量最多的样本长度为 seq_len\n",
        "        # 令:一个batch中某个单词长度最长的单词长度为 word_len\n",
        "        # batch.c_char: [batch,seq_len,word_len] 后两个维度对应context\n",
        "        # c_char: [batch, seq_len, char_channel_size]\n",
        "        c_char = char_emb_layer(batch.c_char) \n",
        "        # batch.q_char = (batch,seq_len,word_len) 后两个维度对应question\n",
        "        # q_char = (batch, seq_len, char_channel_size)\n",
        "        q_char = char_emb_layer(batch.q_char)\n",
        "        \n",
        "        # 2. Word Embedding Layer\n",
        "        # batch.c_word[0] = (batch,seq_len) 后一个维度对应context\n",
        "        # c_word = (batch, seq_len, word_dim) word_dim是Glove词向量维度\n",
        "        c_word = self.word_emb(batch.c_word[0])\n",
        "        # batch.q_word[0] = (batch,seq_len) 后一个维度对应question\n",
        "        # q_word = (batch, seq_len, word_dim)\n",
        "        q_word = self.word_emb(batch.q_word[0]) \n",
        "        # c_lens：一个batch中所有context的样本长度\n",
        "        c_lens = batch.c_word[1]\n",
        "        # q_lens：一个batch中所有question的样本长度\n",
        "        q_lens = batch.q_word[1]\n",
        "\n",
        "        # Highway network\n",
        "        # c = (batch, seq_len, hidden_size * 2)\n",
        "        c = highway_network(c_char, c_word)\n",
        "        # q = (batch, seq_len, hidden_size * 2)\n",
        "        q = highway_network(q_char, q_word)\n",
        "        \n",
        "        # 3. Contextual Embedding Layer  这个返回两个变量，简化成一个\n",
        "        # c: [batch, seq_len, hidden_size * 2]\n",
        "        c = self.context_LSTM((c, c_lens))[0]\n",
        "        \n",
        "        # q: [batch, seq_len, hidden_size * 2]\n",
        "        q = self.context_LSTM((q, q_lens))[0]\n",
        "        \n",
        "        # 4. Attention Flow Layer\n",
        "        # [batch, c_len, hidden_size * 8]\n",
        "        g = att_flow_layer(c, q)\n",
        "        \n",
        "        # 5. Modeling Layer\n",
        "        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[0], c_lens))[0]\n",
        "        # self.modeling_LSTM1((g, c_lens))[0] = (batch, c_len, hidden_size * 2) # 2因为是双向\n",
        "        # m = (batch, c_len, hidden_size * 2) 2因为是双向\n",
        "        \n",
        "        # 6. Output Layer\n",
        "        p1, p2 = output_layer(g, m, c_lens) # 预测开始位置和结束位置\n",
        "        # (batch, c_len), (batch, c_len)\n",
        "        return p1, p2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSBgthF3SCAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = BiDAF(args, data.WORD.vocab.vectors).to(args.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmzoqg2cxvBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EMA():\n",
        "    \"\"\" 滑动平均 exponential moving averages \"\"\"\n",
        "    def __init__(self, mu):\n",
        "        # mu = args.exp_decay_rate = 0.999\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self, name, val):\n",
        "        \"\"\"用于记录模型参数\"\"\"\n",
        "        # name:各个参数层的名字, param.data；参数层的数据\n",
        "        self.shadow[name] = val.clone() # 建立字典\n",
        "        # clone()得到的Tensor不仅拷贝了原始的value，而且会计算梯度传播信息，copy_()只拷贝数值\n",
        "\n",
        "    def get(self, name):\n",
        "        return self.shadow[name]\n",
        "\n",
        "    def update(self, name, x):\n",
        "        assert name in self.shadow\n",
        "        new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n",
        "        self.shadow[name] = new_average.clone()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSea883AxvBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, ema, args, data):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = 0\n",
        "    answers = dict()\n",
        "    model.eval()\n",
        "\n",
        "    backup_params = EMA(0)  # 是否不起作用？\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            backup_params.register(name, param.data)\n",
        "            param.data.copy_(ema.get(name))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iter(data.dev_iter)):\n",
        "            p1, p2 = model(batch)\n",
        "            batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "            \n",
        "            loss += batch_loss.item()\n",
        "\n",
        "            # (batch, c_len, c_len)\n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(args.device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                id = batch.id[i]\n",
        "                answer = batch.c_word[0][i][s_idx[i]:e_idx[i]+1]\n",
        "                answer = ' '.join([data.WORD.vocab.itos[idx] for idx in answer])\n",
        "                answers[id] = answer\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(backup_params.get(name))\n",
        "\n",
        "    with open(args.prediction_file, 'w', encoding='utf-8') as f:\n",
        "        print(json.dumps(answers), file=f)\n",
        "\n",
        "    results = evaluate.main(args)\n",
        "\n",
        "    return loss, results['exact_match'], results['f1']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MySFrPmRxvBW",
        "colab_type": "text"
      },
      "source": [
        "训练函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bbhs6GHxvBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, data):\n",
        "    model = BiDAF(args, data.WORD.vocab.vectors).to(args.device) # 定义主模型类实例\n",
        "    # exponential moving averages 初始化\n",
        "    ema = EMA(args.exp_decay_rate) # args.exp_decay_rate = 0.999\n",
        "    for name, param in model.named_parameters(): \n",
        "        if param.requires_grad:\n",
        "            ema.register(name, param.data) # 参数名字和对应的参数数据形成字典\n",
        "    # p.requires_grad = True or False 保留有梯度的参数\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = optim.Adadelta(parameters, lr=args.learning_rate)\n",
        "    # 交叉熵损失\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # 初始化 tensorboard 记录\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    model.train()\n",
        "    loss, last_epoch = 0, -1\n",
        "    max_dev_exact, max_dev_f1 = -1, -1\n",
        "    global_step = 0\n",
        "    for epoch in range(args.epoch):\n",
        "        logger.info(\"***** Epoch：%d *****\", (epoch + 1))\n",
        "        iterator = data.train_iter\n",
        "        for i, batch in enumerate(iterator):\n",
        "            # (batch, c_len), (batch, c_len)\n",
        "            p1, p2 = model(batch)\n",
        "            optimizer.zero_grad()\n",
        "            # 最后的目标函数：batch.s_idx是答案开始的位置，batch.e_idx是答案结束的位置\n",
        "            batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "            \n",
        "            loss += batch_loss.item()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "            # exponential moving averages\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    ema.update(name, param.data) # 更新训练完后的的参数数据\n",
        "\n",
        "            global_step += 1\n",
        "            if (i + 1) % args.logging_steps == 0:\n",
        "                c = (i + 1) // args.logging_steps\n",
        "                logger.info(\"Step: {} Training loss：{:.4f}\".format(i+1, loss))\n",
        "                writer.add_scalar('loss/train', loss, global_step)\n",
        "                loss = 0\n",
        "\n",
        "    writer.close()\n",
        "    logger.info(\"Saving model to %s\", args.model_dir)\n",
        "    torch.save(model.state_dict(), args.model_dir)\n",
        "    # 测试模型\n",
        "    dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)\n",
        "    logger.info(f'logging dev EM: {dev_exact:.3f} / max dev F1: {dev_f1:.3f}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8u0kA_xxvBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(args, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Nkmm7ztY8o",
        "colab_type": "text"
      },
      "source": [
        "下载文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK71Z-0btEwf",
        "colab_type": "code",
        "outputId": "f6c439ea-e5e7-437e-f50e-933159b9662a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BiDAF.pkl  data  evaluate.py  __pycache__  runs  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbK9kfpHxvBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "51971bbd-58be-4e21-e48a-90d363d7b429"
      },
      "source": [
        "!tar -zcvf runs.tar.gz runs"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runs/\n",
            "runs/Oct28_01-59-56_0469296fdbb4/\n",
            "runs/Oct28_01-59-56_0469296fdbb4/events.out.tfevents.1572227998.0469296fdbb4.147.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzOmZIuVtjpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# results = evaluate.main(args)\n",
        "# dev EM: 54.13 dev F1:  66.95"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfgWskkb8Tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}