{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "RoBERTa-SWAG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "owc7D-pl5ZCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6p9DNXX5XN7",
        "colab_type": "text"
      },
      "source": [
        "## RoBERTa Fine-tuning on SWAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7p1A3In5XN-",
        "colab_type": "text"
      },
      "source": [
        "参考 transformers example [run_multiple_choice.py](https://github.com/huggingface/transformers/blob/master/examples/run_multiple_choice.py) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdCnpvJL5XOA",
        "colab_type": "text"
      },
      "source": [
        "### SWAG 数据集简介"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8yDKOW_5XOB",
        "colab_type": "text"
      },
      "source": [
        "[详细简介](https://rowanzellers.com/swag/) \n",
        "SWAG (Situations with Adversarial Generations) 是一个用于常识推理的数据集，用于自然语言推理和基于物理推理的任务。主要形式是给出一段描述，例如：\"she opened the hood of the car,\"，根据这段描述推断接下来会发生什么，比如说：\"then, she examined the engine\"。\n",
        "\n",
        "数据集由 113k 个涉及丰富的基础情境的 **多项选择题** 组成。每个问题都是来自  [LSMDC](https://sites.google.com/site/describingmovies/lsmdc-2017) 或 [ActivityNet Captions](https://cs.stanford.edu/people/ranjaykrishna/densevid/)  的视频字幕，有四个选项可以选择接下来场景中会发生什么。正确答案是视频中接下来的字幕；错误的三个答案是经过人工推测的。\n",
        "\n",
        "其中训练集有 73K 个问题，验证集包含 20K 个问题，测试集包含 20K 个没有答案的问题。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4f4b8JM5XOC",
        "colab_type": "text"
      },
      "source": [
        "列名解释：`video-id`,`fold-ind`,两个字段是标识； `startphrase` 为整个段落，段落被分割为两部分  `sent1`,`sent2` ；每个段落还包含4个结尾 `ending0, ending1, ending2, ending3` ；每个段落的正确结尾用 `label` 指明。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPOgMFsS5XOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "# from io import open\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from typing import List\n",
        "from transformers import PreTrainedTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0pvbKFV5XOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import (DataLoader, RandomSampler, \n",
        "                SequentialSampler, TensorDataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaM0mGAr5XOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import (WEIGHTS_NAME, \n",
        "#     BertConfig, BertForMultipleChoice, BertTokenizer,\n",
        "#     XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer, \n",
        "    RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer)\n",
        "from transformers import AdamW, WarmupLinearSchedule\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n93bY005XOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eAjLuX5XOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, XLNetConfig, RobertaConfig)), ())\n",
        "MODEL_CLASSES = {\n",
        "#     'bert': (BertConfig, BertForMultipleChoice, BertTokenizer),\n",
        "#     'xlnet': (XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwuFWkoO5XOO",
        "colab_type": "text"
      },
      "source": [
        "### 设置参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfWWBIT5XOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments(object):\n",
        "    do_train = True  # Whether to run training.\n",
        "    do_eval = True  # Whether to run eval on the dev set.\n",
        "    do_test = True  # Whether to run test on the test set\n",
        "    task_name = 'swag'  # The name of the task to train\n",
        "    model_type = 'roberta'  # Model type\n",
        "    \n",
        "    # 模型参数\n",
        "    config_name = ''  # Pretrained config name or path if not the same as model_name\n",
        "    tokenizer_name = ''  # Pretrained tokenizer name or path if not the same as model_name\n",
        "    model_name_or_path = 'roberta-base'  # Path to pre-trained model or shortcut name\n",
        "    do_lower_case = True  # Set this flag if you are using an uncased model.\n",
        "    \n",
        "    # 数据参数\n",
        "    data_dir = './data'  # The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
        "    max_seq_length = 80  # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "    overwrite_cache = True  # Overwrite the cached training and evaluation sets\n",
        "    output_dir = 'models_bert/swag_base'  # The output directory where the model predictions and checkpoints will be written.\n",
        "    overwrite_output_dir = True  # Overwrite the content of the output directory\n",
        "    \n",
        "    # 训练参数\n",
        "    max_steps = -1  # If > 0: set total number of training steps to perform. Override num_train_epochs.\n",
        "    num_train_epochs = 3  # Total number of training epochs to perform.\n",
        "    train_batch_size = 16  # Batch size per GPU/CPU for training.\n",
        "    eval_batch_size = 16  # Batch size per GPU/CPU for evaluation.\n",
        "    gradient_accumulation_steps = 2  # Number of updates steps to accumulate before performing a backward/update pass.\n",
        "    weight_decay = 0.0  # Weight deay if we apply some.\n",
        "    learning_rate = 5e-5  # The initial learning rate for Adam.\n",
        "    adam_epsilon = 1e-8  # Epsilon for Adam optimizer.\n",
        "    warmup_steps = 0  # Linear warmup over warmup_steps.\n",
        "    max_grad_norm = 1.0  # Max gradient norm.\n",
        "    \n",
        "    logging_steps = 100  # Log every X updates steps.\n",
        "    evaluate_during_training = True  # Run evaluation during training at each logging step.\n",
        "    save_steps = 100  # Save checkpoint every X updates steps.\n",
        "    \n",
        "    fp16 = False  # Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n",
        "    fp16_opt_level = 'O1'  # For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\n",
        "    # Setup CUDA\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "args = Arguments()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2A89N2j5XOP",
        "colab_type": "text"
      },
      "source": [
        "### 数据处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMnrguH05XOP",
        "colab_type": "text"
      },
      "source": [
        "样本类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0igJy0B5XOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for multiple choice\"\"\"\n",
        "\n",
        "    def __init__(self, example_id, question, contexts, endings, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            example_id: Unique id for the example.\n",
        "            contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n",
        "            question: string. The untokenized text of the second sequence (question).\n",
        "            endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.example_id = example_id\n",
        "        self.question = question\n",
        "        self.contexts = contexts\n",
        "        self.endings = endings\n",
        "        self.label = label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uezZTvOZ5XOR",
        "colab_type": "text"
      },
      "source": [
        "数据处理基类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na1HdXaR5XOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wW5bnmB5XOV",
        "colab_type": "text"
      },
      "source": [
        "SWAG 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_9T_G2L5XOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SwagProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the SWAG data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
        "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
        "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
        "        raise ValueError(\n",
        "            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n",
        "            \"setting!\"\n",
        "        )\n",
        "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\", \"2\", \"3\"]\n",
        "\n",
        "    def _read_csv(self, input_file):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "    def _create_examples(self, lines: List[List[str]], type: str):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        if type == \"train\" and lines[0][-1] != 'label':\n",
        "            raise ValueError(\n",
        "                \"For training, the input file must contain a label column.\"\n",
        "            )\n",
        "\n",
        "        examples = [\n",
        "            InputExample(\n",
        "                example_id=line[2],\n",
        "                # in the swag dataset, the common beginning of each choice is stored in \"sent2\".\n",
        "                question=line[5],  \n",
        "                # 将四个答案构成四对 context-ending\n",
        "                contexts = [line[4], line[4], line[4], line[4]],\n",
        "                endings = [line[7], line[8], line[9], line[10]],\n",
        "                label=line[11]\n",
        "            ) for line in lines[1:]  # we skip the line with the column names\n",
        "        ]\n",
        "\n",
        "        return examples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0WqC5i_5XOX",
        "colab_type": "text"
      },
      "source": [
        "模型输入 feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QehUMsbT5XOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    def __init__(self, example_id, choices_features, label):\n",
        "        self.example_id = example_id\n",
        "        self.choices_features = [\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'segment_ids': segment_ids\n",
        "            }\n",
        "            for input_ids, input_mask, segment_ids in choices_features\n",
        "        ]\n",
        "        self.label = label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXkn3LKv5XOa",
        "colab_type": "text"
      },
      "source": [
        "将样本转换为模型输入feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l-PcIVq5XOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(\n",
        "    examples: List[InputExample],\n",
        "    label_list: List[str],\n",
        "    max_length: int,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    pad_token_segment_id=0,\n",
        "    pad_on_left=False,\n",
        "    pad_token=0,\n",
        "    mask_padding_with_zero=True,\n",
        ") -> List[InputFeatures]:\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of `InputFeatures`\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in tqdm(enumerate(examples), desc=\"convert examples to features\"):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "        choices_features = []\n",
        "        # 迭代 example 中的每个 context-ending 对\n",
        "        for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):\n",
        "            text_a = context\n",
        "            if example.question.find(\"_\") != -1:\n",
        "                # this is for cloze question 用于完形填空题\n",
        "                text_b = example.question.replace(\"_\", ending)\n",
        "            else:\n",
        "                text_b = example.question + \" \" + ending\n",
        "            # 可以\n",
        "            inputs = tokenizer.encode_plus(\n",
        "                text_a,\n",
        "                text_b,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "            )\n",
        "            if 'num_truncated_tokens' in inputs and inputs['num_truncated_tokens'] > 0:\n",
        "                logger.info('Attention! you are cropping tokens (swag task is ok). '\n",
        "                        'If you are training ARC and RACE and you are poping question + options,'\n",
        "                        'you need to try to use a bigger max seq length!')\n",
        "\n",
        "            input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.  padding！！！！！\n",
        "            padding_length = max_length - len(input_ids)\n",
        "            if pad_on_left:\n",
        "                input_ids = ([pad_token] * padding_length) + input_ids\n",
        "                attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
        "                token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
        "            else:\n",
        "                input_ids = input_ids + ([pad_token] * padding_length)\n",
        "                attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "                token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "            assert len(input_ids) == max_length\n",
        "            assert len(attention_mask) == max_length\n",
        "            assert len(token_type_ids) == max_length\n",
        "            choices_features.append((input_ids, attention_mask, token_type_ids))\n",
        "\n",
        "        label = label_map[example.label]\n",
        "\n",
        "        if ex_index < 2:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"race_id: {}\".format(example.example_id))\n",
        "            for choice_idx, (input_ids, attention_mask, token_type_ids) in enumerate(choices_features):\n",
        "                logger.info(\"choice: {}\".format(choice_idx))\n",
        "                logger.info(\"input_ids: {}\".format(' '.join(map(str, input_ids))))\n",
        "                logger.info(\"attention_mask: {}\".format(' '.join(map(str, attention_mask))))\n",
        "                logger.info(\"token_type_ids: {}\".format(' '.join(map(str, token_type_ids))))\n",
        "                logger.info(\"label: {}\".format(label))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                example_id=example.example_id,\n",
        "                choices_features=choices_features,\n",
        "                label=label,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7roHhx65XOd",
        "colab_type": "text"
      },
      "source": [
        "数据处理集合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC002BH45XOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processors = {\n",
        "#     \"race\": RaceProcessor,\n",
        "    \"swag\": SwagProcessor,\n",
        "#     \"arc\": ArcProcessor\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48jkmSFx5XOg",
        "colab_type": "text"
      },
      "source": [
        "选择 feature 中的 field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvzEuwFs5XOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_field(features, field):\n",
        "    return [\n",
        "        [\n",
        "            choice[field]\n",
        "            for choice in feature.choices_features\n",
        "        ]\n",
        "        for feature in features\n",
        "    ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEjgtYwn5XOi",
        "colab_type": "text"
      },
      "source": [
        "加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9fWEEzf5XOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(args, task, tokenizer, evaluate=False, test=False):\n",
        "    processor = processors[task]()\n",
        "    # Load data features from cache or dataset file\n",
        "    if evaluate:\n",
        "        cached_mode = 'dev'\n",
        "    elif test:\n",
        "        cached_mode = 'test'\n",
        "    else:\n",
        "        cached_mode = 'train'\n",
        "    assert (evaluate == True and test == True) == False\n",
        "    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n",
        "        cached_mode,\n",
        "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
        "        str(args.max_seq_length),\n",
        "        str(task)))\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "        if evaluate:\n",
        "            examples = processor.get_dev_examples(args.data_dir)\n",
        "        elif test:\n",
        "            examples = processor.get_test_examples(args.data_dir)\n",
        "        else:\n",
        "            examples = processor.get_train_examples(args.data_dir)\n",
        "        logger.info(\"Training number: %s\", str(len(examples)))\n",
        "        features = convert_examples_to_features(\n",
        "            examples,\n",
        "            label_list,\n",
        "            args.max_seq_length,\n",
        "            tokenizer,\n",
        "            pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n",
        "            pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor(select_field(features, 'input_ids'), dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(select_field(features, 'input_mask'), dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(select_field(features, 'segment_ids'), dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    \n",
        "    return dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvnVUm8z5XOl",
        "colab_type": "text"
      },
      "source": [
        "### 训练模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuyU1R4an3kz",
        "colab_type": "text"
      },
      "source": [
        "计算准确率"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9MPPs_Gn7b5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-x70RPE5XOp",
        "colab_type": "text"
      },
      "source": [
        "测试函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWNVny3o5XOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\", test=False):\n",
        "    eval_task_names = (args.task_name,)\n",
        "    eval_outputs_dirs = (args.output_dir,)\n",
        "\n",
        "    results = {}\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=not test, test=test)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir):\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        # Note that DistributedSampler samples randomly\n",
        "        eval_sampler = SequentialSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids':      batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                          'labels':         batch[3]}\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        acc = simple_accuracy(preds, out_label_ids)\n",
        "        result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n",
        "        results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(eval_output_dir, \"is_test_\" + str(test).lower() + \"_eval_results.txt\")\n",
        "\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results {} *****\".format(str(prefix) + \" is test:\" + str(test)))\n",
        "            writer.write(\"model           =%s\\n\" % str(args.model_name_or_path))\n",
        "            writer.write(\"total batch size=%d\\n\" % (args.train_batch_size * args.gradient_accumulation_steps))\n",
        "            writer.write(\"train num epochs=%d\\n\" % args.num_train_epochs)\n",
        "            writer.write(\"fp16            =%s\\n\" % args.fp16)\n",
        "            writer.write(\"max seq length  =%d\\n\" % args.max_seq_length)\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "    return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MUDWBFL5XOt",
        "colab_type": "text"
      },
      "source": [
        "训练函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypvf24_G5XOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    tb_writer = SummaryWriter()\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    best_dev_acc, best_dev_loss = 0.0, 99999999999.0\n",
        "    best_steps = 0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=True)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                        if results[\"eval_acc\"] > best_dev_acc:\n",
        "                            best_dev_acc = results[\"eval_acc\"]\n",
        "                            best_dev_loss = results[\"eval_loss\"]\n",
        "                            best_steps = global_step\n",
        "                            if args.do_test:\n",
        "                                results_test = evaluate(args, model, tokenizer, test=True)\n",
        "                                for key, value in results_test.items():\n",
        "                                    tb_writer.add_scalar('test_{}'.format(key), value, global_step)\n",
        "                                logger.info(\"test acc: %s, loss: %s, global steps: %s\", str(results_test['eval_acc']), str(results_test['eval_loss']), str(global_step))\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
        "                    logger.info(\"Average loss: %s at global step: %s\", str((tr_loss - logging_loss)/args.logging_steps), str(global_step))\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_vocabulary(output_dir)\n",
        "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step, best_steps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6RbxQqg5XOw",
        "colab_type": "text"
      },
      "source": [
        "主函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk5xGTZA5XOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "    # 模型预测及检查点保存位置\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt = '%m/%d/%Y %H:%M:%S', level = logging.INFO)\n",
        "\n",
        "    # Prepare GLUE task  准备数据\n",
        "    args.task_name = args.task_name.lower()\n",
        "    if args.task_name not in processors:\n",
        "        raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
        "    processor = processors[args.task_name]()\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "    # 加载模型\n",
        "    args.model_type = args.model_type.lower()\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name)\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    best_steps = 0\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
        "        global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "        # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(args.output_dir):\n",
        "            os.makedirs(args.output_dir)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = model_class.from_pretrained(args.output_dir)\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        if not args.do_train:\n",
        "            args.output_dir = args.model_name_or_path\n",
        "        checkpoints = [args.output_dir]\n",
        "        # if args.eval_all_checkpoints:  # 减少运算\n",
        "        #     checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "        #     logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
        "            \n",
        "            model = model_class.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
        "            result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    if args.do_test:\n",
        "        if not args.do_train:\n",
        "            args.output_dir = args.model_name_or_path\n",
        "        checkpoints = [args.output_dir]\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
        "            \n",
        "            model = model_class.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, prefix=prefix, test=True)\n",
        "            result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "    if best_steps:\n",
        "        logger.info(\"best steps of eval acc is the following checkpoints: %s\", best_steps)\n",
        "    return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbF3_9Pt5XOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUqkDhdVTuQS",
        "colab_type": "text"
      },
      "source": [
        "下载文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frx72D0U5XO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b4c9672-65d8-48b3-a307-844e550349a1"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  models_bert  runs  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybxHXvQtTtcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "90c09a85-c85e-45c4-bcba-93c268b43c50"
      },
      "source": [
        "!tar -zcvf test.tar.gz runs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runs/\n",
            "runs/Oct27_01-57-14_8fe654104a8e/\n",
            "runs/Oct27_01-57-14_8fe654104a8e/events.out.tfevents.1572141434.8fe654104a8e.124.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSi3MItan_6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eval_acc = 0.8213036089173248"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}